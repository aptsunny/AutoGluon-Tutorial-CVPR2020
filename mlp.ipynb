{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Advanced HPO Algorithms\n",
    "\n",
    ":label:`sec_custom_advancedhpo`\n",
    "\n",
    "\n",
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Basic utils for folder manipulations etc\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "import multiprocessing # to count the number of CPUs available\n",
    "\n",
    "# External tools to load and process data\n",
    "import openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# MXNet (NeuralNets)\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "# AutoGluon and HPO tools\n",
    "import autogluon as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the version of MxNet, you should be fine with version >= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the version of AutoGluon and the specific commit and check that it matches what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.11b20200614'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization of a 2-layer MLP\n",
    "\n",
    "### Setting up the context\n",
    "\n",
    "Here\n",
    "we declare a few \"environment variables\" setting the context for what we're\n",
    "doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "OPENML_TASK_ID = 6                # describes the problem we will tackle\n",
    "RATIO_TRAIN_VALID = 0.33          # split of the training data used for validation\n",
    "RESOURCE_ATTR_NAME = 'epoch'      # how do we measure resources   (will become clearer further)\n",
    "REWARD_ATTR_NAME = 'objective'    # how do we measure performance (will become clearer further)\n",
    "\n",
    "NUM_CPUS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Set the openml directory to the current directory (next to the notebook) and download the task if the file is not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved dataset 6: letter to file ./org/openml/www/datasets/6/dataset.pkl.py3\n"
     ]
    }
   ],
   "source": [
    "openml.config.set_cache_directory(\"./\")\n",
    "task = openml.tasks.get_task(OPENML_TASK_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a multiclass classification task with 26 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(task.class_labels)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenML provides a standard train/test split that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices, test_indices = task.get_train_test_split_indices()\n",
    "\n",
    "X, y = task.get_X_and_y()\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute any missing value with a basic strategy and recover the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resplit the training data into training+validation using the specific fraction indicated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X_train, y_train, random_state=1, test_size=RATIO_TRAIN_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean) / (std + 1e-10)\n",
    "X_valid = (X_valid - mean) / (std + 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring a model specifying a hyperparameter space with AutoGluon\n",
    "\n",
    "Two layer MLP where we optimize over:\n",
    "\n",
    "- the number of units on the first layer\n",
    "- the number of units on the second layer\n",
    "- the dropout rate after each layer\n",
    "- the learning rate\n",
    "- the scaling\n",
    "- the `@ag.args` decorator allows us to specify the space we will optimize over, this matches the [ConfigSpace](https://automl.github.io/ConfigSpace/master/) syntax\n",
    "\n",
    "The body of the function `run_mlp_openml` is pretty simple:\n",
    "\n",
    "- it reads the hyperparameters given via the decorator\n",
    "- it defines a 2 layer MLP with dropout\n",
    "- it declares a trainer with the 'adam' loss function and a provided learning rate\n",
    "- it trains the NN with a number of epochs (most of that is boilerplate code from `mxnet`)\n",
    "- the `reporter` at the end is used to keep track of training history in the hyperparameter optimization\n",
    "\n",
    "**Note**: The number of epochs and the hyperparameter space are reduced to make for a shorter experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "@ag.args(n_units_1=ag.space.Int(lower=16, upper=128),\n",
    "         n_units_2=ag.space.Int(lower=16, upper=128),\n",
    "         dropout_1=ag.space.Real(lower=0, upper=.75),\n",
    "         dropout_2=ag.space.Real(lower=0, upper=.75),\n",
    "         learning_rate=ag.space.Real(lower=1e-6, upper=1, log=True),\n",
    "         batch_size=ag.space.Int(lower=8, upper=128),\n",
    "         scale_1=ag.space.Real(lower=0.001, upper=10, log=True),\n",
    "         scale_2=ag.space.Real(lower=0.001, upper=10, log=True),\n",
    "         epochs=9)\n",
    "def run_mlp_openml(args, reporter, **kwargs):\n",
    "    # Time stamp for elapsed_time\n",
    "    ts_start = time.time()\n",
    "    # Unwrap hyperparameters\n",
    "    n_units_1 = args.n_units_1\n",
    "    n_units_2 = args.n_units_2\n",
    "    dropout_1 = args.dropout_1\n",
    "    dropout_2 = args.dropout_2\n",
    "    scale_1 = args.scale_1\n",
    "    scale_2 = args.scale_2\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "\n",
    "    ctx = mx.cpu()\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        # Layer 1\n",
    "        net.add(nn.Dense(n_units_1, activation='relu',\n",
    "                         weight_initializer=mx.initializer.Uniform(scale=scale_1)))\n",
    "        # Dropout\n",
    "        net.add(gluon.nn.Dropout(dropout_1))\n",
    "        # Layer 2\n",
    "        net.add(nn.Dense(n_units_2, activation='relu',\n",
    "                         weight_initializer=mx.initializer.Uniform(scale=scale_2)))\n",
    "        # Dropout\n",
    "        net.add(gluon.nn.Dropout(dropout_2))\n",
    "        # Output\n",
    "        net.add(nn.Dense(n_classes))\n",
    "    net.initialize(ctx=ctx)\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate})\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        ts_epoch = time.time()\n",
    "\n",
    "        train_iter = mx.io.NDArrayIter(\n",
    "                        data={'data': X_train}, \n",
    "                        label={'label': y_train},\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "        valid_iter = mx.io.NDArrayIter(\n",
    "                        data={'data': X_valid}, \n",
    "                        label={'label': y_valid},\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False)\n",
    "\n",
    "        metric = mx.metric.Accuracy()\n",
    "        loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "        for batch in train_iter:\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                L = loss(output, label)\n",
    "            L.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            metric.update([label], [output])\n",
    "\n",
    "        name, train_acc = metric.get()\n",
    "\n",
    "        metric = mx.metric.Accuracy()\n",
    "        for batch in valid_iter:\n",
    "            data = batch.data[0].as_in_context(ctx)\n",
    "            label = batch.label[0].as_in_context(ctx)\n",
    "            output = net(data)\n",
    "            metric.update([label], [output])\n",
    "\n",
    "        name, val_acc = metric.get()\n",
    "\n",
    "        print('Epoch %d ; Time: %f ; Training: %s=%f ; Validation: %s=%f' % (\n",
    "            epoch + 1, time.time() - ts_start, name, train_acc, name, val_acc))\n",
    "\n",
    "        ts_now = time.time()\n",
    "        eval_time = ts_now - ts_epoch\n",
    "        elapsed_time = ts_now - ts_start\n",
    "\n",
    "        # The resource reported back (as 'epoch') is the number of epochs\n",
    "        # done, starting at 1\n",
    "        reporter(\n",
    "            epoch=epoch + 1, \n",
    "            objective=float(val_acc), \n",
    "            eval_time=eval_time,\n",
    "            time_step=ts_now, \n",
    "            elapsed_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The annotation `epochs=9` specifies the maximum number of epochs for\n",
    "training. It becomes available as `args.epochs`. Importantly, it is also\n",
    "processed by `HyperbandScheduler` below in order to set its `max_t` attribute.\n",
    "\n",
    "**Recommendation**: Whenever writing training code to be passed as `train_fn` to\n",
    "a scheduler, if this training code reports a resource (or time) attribute, the\n",
    "corresponding maximum resource value should be included in `train_fn.args`:\n",
    "\n",
    "- If the resource attribute (`time_attr` of scheduler) in `train_fn` is `epoch`,\n",
    "  make sure to include `epochs=XYZ` in the annotation. This allows the scheduler\n",
    "  to read `max_t` from `train_fn.args.epochs`. This case corresponds to our\n",
    "  example here.\n",
    "- If the resource attribute is something else than `epoch`, you can also include\n",
    "  the annotation `max_t=XYZ`, which allows the scheduler to read `max_t` from\n",
    "  `train_fn.args.max_t`.\n",
    "\n",
    "Annotating the training function by the correct value for `max_t` simplifies\n",
    "scheduler creation (since `max_t` does not have to be passed), and avoids\n",
    "inconsistencies between `train_fn` and the scheduler.\n",
    "\n",
    "\n",
    "### Running the Hyperparameter Optimization\n",
    "\n",
    "You can use the following schedulers:\n",
    "\n",
    "- FIFO (`fifo`)\n",
    "- Hyperband (either the stopping (`hbs`) or promotion (`hbp`) variant)\n",
    "\n",
    "And the following searchers:\n",
    "\n",
    "- Random search (`random`)\n",
    "- Gaussian process based Bayesian optimization (`bayesopt`)\n",
    "- SkOpt Bayesian optimization (`skopt`; only with FIFO scheduler)\n",
    "\n",
    "Note that the method known as (asynchronous) Hyperband is using random search.\n",
    "Combining Hyperband scheduling with the `bayesopt` searcher uses a novel\n",
    "method called asynchronous BOHB.\n",
    "\n",
    "Pick the combination you're interested in (doing the full experiment takes around\n",
    "120 seconds, see the `time_out` parameter), running everything with multiple runs\n",
    "can take a fair bit of time. In real life, you will want to choose a larger\n",
    "`time_out` in order to obtain good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "37"
    }
   },
   "outputs": [],
   "source": [
    "SCHEDULER = \"hbs\"\n",
    "SEARCHER = \"bayesopt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "38"
    }
   },
   "outputs": [],
   "source": [
    "def compute_error(df):\n",
    "    return 1.0 - df[\"objective\"]\n",
    "\n",
    "def compute_runtime(df, start_timestamp):\n",
    "        return df[\"time_step\"] - start_timestamp\n",
    "\n",
    "def process_training_history(task_dicts, start_timestamp, \n",
    "                             runtime_fn=compute_runtime,\n",
    "                             error_fn=compute_error):\n",
    "    task_dfs = []\n",
    "    for task_id in task_dicts:\n",
    "        task_df = pd.DataFrame(task_dicts[task_id])\n",
    "        task_df = task_df.assign(task_id=task_id,\n",
    "                                 runtime=runtime_fn(task_df, start_timestamp),\n",
    "                                 error=error_fn(task_df),\n",
    "                                 target_epoch=task_df[\"epoch\"].iloc[-1])\n",
    "        task_dfs.append(task_df)\n",
    "\n",
    "    result = pd.concat(task_dfs, axis=\"index\", ignore_index=True, sort=True)\n",
    "    # re-order by runtime\n",
    "    result = result.sort_values(by=\"runtime\")\n",
    "    # calculate incumbent best -- the cumulative minimum of the error.\n",
    "    result = result.assign(best=result[\"error\"].cummin())\n",
    "    return result\n",
    "\n",
    "resources = dict(num_cpus=NUM_CPUS, num_gpus=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "39"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_t = 9, as inferred from train_fn.args\n",
      "search_options: Key 'random_seed': Imputing default value 31415927\n",
      "search_options: Key 'opt_skip_init_length': Imputing default value 150\n",
      "search_options: Key 'opt_skip_period': Imputing default value 1\n",
      "search_options: Key 'profiler': Imputing default value False\n",
      "search_options: Key 'opt_maxiter': Imputing default value 50\n",
      "search_options: Key 'opt_nstarts': Imputing default value 2\n",
      "search_options: Key 'opt_warmstart': Imputing default value False\n",
      "search_options: Key 'opt_verbose': Imputing default value False\n",
      "search_options: Key 'opt_debug_writer': Imputing default value False\n",
      "search_options: Key 'num_fantasy_samples': Imputing default value 20\n",
      "search_options: Key 'num_init_random': Imputing default value 10\n",
      "search_options: Key 'num_init_candidates': Imputing default value 250\n",
      "search_options: Key 'initial_scoring': Imputing default value thompson_indep\n",
      "search_options: Key 'first_is_default': Imputing default value True\n",
      "search_options: Key 'debug_log': Imputing default value False\n",
      "search_options: Key 'opt_skip_num_max_resource': Imputing default value False\n",
      "search_options: Key 'gp_resource_kernel': Imputing default value matern52\n",
      "search_options: Key 'resource_acq': Imputing default value bohb\n",
      "\n",
      "Starting Experiments\n",
      "Num of Finished Tasks is 0\n",
      "Time out (secs) is 120\n",
      "/home/ec2-user/anaconda3/envs/autogluon/lib/python3.7/site-packages/distributed/worker.py:3378: UserWarning: Large object of size 1.30 MB detected in task graph: \n",
      "  (<function run_mlp_openml at 0x7fab1b6417a0>, {'ar ... sReporter}, [])\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.595077 ; Training: accuracy=0.260079 ; Validation: accuracy=0.531250\n",
      "Epoch 2 ; Time: 1.171749 ; Training: accuracy=0.496365 ; Validation: accuracy=0.655247\n",
      "Epoch 3 ; Time: 1.740634 ; Training: accuracy=0.559650 ; Validation: accuracy=0.694686\n",
      "Epoch 4 ; Time: 2.288682 ; Training: accuracy=0.588896 ; Validation: accuracy=0.711063\n",
      "Epoch 5 ; Time: 2.838102 ; Training: accuracy=0.609385 ; Validation: accuracy=0.726939\n",
      "Epoch 6 ; Time: 3.373529 ; Training: accuracy=0.628139 ; Validation: accuracy=0.745321\n",
      "Epoch 7 ; Time: 3.924655 ; Training: accuracy=0.641193 ; Validation: accuracy=0.750501\n",
      "Epoch 8 ; Time: 4.482823 ; Training: accuracy=0.653751 ; Validation: accuracy=0.763202\n",
      "Epoch 9 ; Time: 5.051333 ; Training: accuracy=0.665482 ; Validation: accuracy=0.766043\n",
      "Epoch 1 ; Time: 0.779395 ; Training: accuracy=0.048005 ; Validation: accuracy=0.111743\n",
      "Epoch 1 ; Time: 0.325519 ; Training: accuracy=0.039835 ; Validation: accuracy=0.063967\n",
      "Epoch 1 ; Time: 0.383138 ; Training: accuracy=0.377145 ; Validation: accuracy=0.688538\n",
      "Epoch 2 ; Time: 0.784324 ; Training: accuracy=0.565594 ; Validation: accuracy=0.739386\n",
      "Epoch 3 ; Time: 1.177280 ; Training: accuracy=0.601073 ; Validation: accuracy=0.750965\n",
      "Epoch 4 ; Time: 1.560595 ; Training: accuracy=0.633993 ; Validation: accuracy=0.764558\n",
      "Epoch 5 ; Time: 1.988534 ; Training: accuracy=0.642657 ; Validation: accuracy=0.784528\n",
      "Epoch 6 ; Time: 2.351279 ; Training: accuracy=0.662789 ; Validation: accuracy=0.781507\n",
      "Epoch 7 ; Time: 2.725280 ; Training: accuracy=0.668647 ; Validation: accuracy=0.800134\n",
      "Epoch 8 ; Time: 3.102319 ; Training: accuracy=0.672112 ; Validation: accuracy=0.806008\n",
      "Epoch 9 ; Time: 3.478864 ; Training: accuracy=0.679785 ; Validation: accuracy=0.805504\n",
      "Epoch 1 ; Time: 0.545670 ; Training: accuracy=0.048280 ; Validation: accuracy=0.036981\n",
      "Epoch 1 ; Time: 2.422754 ; Training: accuracy=0.098507 ; Validation: accuracy=0.047138\n",
      "Epoch 1 ; Time: 0.814200 ; Training: accuracy=0.098926 ; Validation: accuracy=0.347395\n",
      "Epoch 2 ; Time: 1.548275 ; Training: accuracy=0.152314 ; Validation: accuracy=0.468739\n",
      "Epoch 3 ; Time: 2.270405 ; Training: accuracy=0.182975 ; Validation: accuracy=0.506891\n",
      "Epoch 1 ; Time: 0.475025 ; Training: accuracy=0.041336 ; Validation: accuracy=0.044098\n",
      "Epoch 1 ; Time: 0.373347 ; Training: accuracy=0.086230 ; Validation: accuracy=0.174115\n",
      "Epoch 1 ; Time: 0.305789 ; Training: accuracy=0.564227 ; Validation: accuracy=0.763630\n",
      "Epoch 2 ; Time: 0.630638 ; Training: accuracy=0.766365 ; Validation: accuracy=0.825632\n",
      "Epoch 3 ; Time: 0.937207 ; Training: accuracy=0.822204 ; Validation: accuracy=0.863697\n",
      "Epoch 4 ; Time: 1.252353 ; Training: accuracy=0.853207 ; Validation: accuracy=0.885638\n",
      "Epoch 5 ; Time: 1.568912 ; Training: accuracy=0.870066 ; Validation: accuracy=0.894947\n",
      "Epoch 6 ; Time: 1.859166 ; Training: accuracy=0.883964 ; Validation: accuracy=0.907580\n",
      "Epoch 7 ; Time: 2.167680 ; Training: accuracy=0.891859 ; Validation: accuracy=0.912400\n",
      "Epoch 8 ; Time: 2.482489 ; Training: accuracy=0.904523 ; Validation: accuracy=0.914062\n",
      "Epoch 9 ; Time: 2.804430 ; Training: accuracy=0.910280 ; Validation: accuracy=0.920545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.23636968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.778011 ; Training: accuracy=0.384711 ; Validation: accuracy=0.618487\n",
      "Epoch 2 ; Time: 1.510047 ; Training: accuracy=0.597190 ; Validation: accuracy=0.687899\n",
      "Epoch 3 ; Time: 2.227235 ; Training: accuracy=0.654298 ; Validation: accuracy=0.749580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.23636968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.418063 ; Training: accuracy=0.242812 ; Validation: accuracy=0.544525\n",
      "Epoch 2 ; Time: 0.836094 ; Training: accuracy=0.381196 ; Validation: accuracy=0.611940\n",
      "Epoch 3 ; Time: 1.255888 ; Training: accuracy=0.424736 ; Validation: accuracy=0.645816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.23636968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.583567 ; Training: accuracy=0.490975 ; Validation: accuracy=0.697980\n",
      "Epoch 2 ; Time: 1.159561 ; Training: accuracy=0.743997 ; Validation: accuracy=0.798822\n",
      "Epoch 3 ; Time: 1.724998 ; Training: accuracy=0.811558 ; Validation: accuracy=0.832660\n",
      "Epoch 4 ; Time: 2.309571 ; Training: accuracy=0.848982 ; Validation: accuracy=0.856566\n",
      "Epoch 5 ; Time: 2.861516 ; Training: accuracy=0.879202 ; Validation: accuracy=0.878283\n",
      "Epoch 6 ; Time: 3.452272 ; Training: accuracy=0.889303 ; Validation: accuracy=0.888047\n",
      "Epoch 7 ; Time: 4.041467 ; Training: accuracy=0.904537 ; Validation: accuracy=0.893771\n",
      "Epoch 8 ; Time: 4.639382 ; Training: accuracy=0.909588 ; Validation: accuracy=0.905051\n",
      "Epoch 9 ; Time: 5.213415 ; Training: accuracy=0.921427 ; Validation: accuracy=0.910606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.23636968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.694321 ; Training: accuracy=0.067769 ; Validation: accuracy=0.159596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.23636968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.426386 ; Training: accuracy=0.424897 ; Validation: accuracy=0.649866\n",
      "Epoch 2 ; Time: 0.876026 ; Training: accuracy=0.495782 ; Validation: accuracy=0.694556\n",
      "Epoch 3 ; Time: 1.304327 ; Training: accuracy=0.516625 ; Validation: accuracy=0.685820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.13630319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 2.239761 ; Training: accuracy=0.553879 ; Validation: accuracy=0.784610\n",
      "Epoch 2 ; Time: 4.478283 ; Training: accuracy=0.711124 ; Validation: accuracy=0.834005\n",
      "Epoch 3 ; Time: 6.845206 ; Training: accuracy=0.753647 ; Validation: accuracy=0.850974\n",
      "Epoch 4 ; Time: 9.141582 ; Training: accuracy=0.780587 ; Validation: accuracy=0.865087\n",
      "Epoch 5 ; Time: 11.446277 ; Training: accuracy=0.795590 ; Validation: accuracy=0.886761\n",
      "Epoch 6 ; Time: 13.779219 ; Training: accuracy=0.814324 ; Validation: accuracy=0.894657\n",
      "Epoch 7 ; Time: 16.038917 ; Training: accuracy=0.819712 ; Validation: accuracy=0.899530\n",
      "Epoch 8 ; Time: 18.339454 ; Training: accuracy=0.829244 ; Validation: accuracy=0.900538\n",
      "Epoch 9 ; Time: 20.650641 ; Training: accuracy=0.834964 ; Validation: accuracy=0.906082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.13630319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.794149 ; Training: accuracy=0.518267 ; Validation: accuracy=0.774916\n",
      "Epoch 2 ; Time: 1.617927 ; Training: accuracy=0.686890 ; Validation: accuracy=0.824080\n",
      "Epoch 3 ; Time: 2.459448 ; Training: accuracy=0.725244 ; Validation: accuracy=0.833779\n",
      "Epoch 4 ; Time: 3.406224 ; Training: accuracy=0.744503 ; Validation: accuracy=0.847993\n",
      "Epoch 5 ; Time: 4.247401 ; Training: accuracy=0.757233 ; Validation: accuracy=0.853344\n",
      "Epoch 6 ; Time: 5.042738 ; Training: accuracy=0.767565 ; Validation: accuracy=0.866890\n",
      "Epoch 7 ; Time: 5.805382 ; Training: accuracy=0.772938 ; Validation: accuracy=0.873579\n",
      "Epoch 8 ; Time: 6.648229 ; Training: accuracy=0.785088 ; Validation: accuracy=0.871070\n",
      "Epoch 9 ; Time: 7.497322 ; Training: accuracy=0.778641 ; Validation: accuracy=0.879431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.13630319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.322368 ; Training: accuracy=0.578795 ; Validation: accuracy=0.775000\n",
      "Epoch 2 ; Time: 0.664241 ; Training: accuracy=0.737789 ; Validation: accuracy=0.822833\n",
      "Epoch 3 ; Time: 1.012690 ; Training: accuracy=0.784323 ; Validation: accuracy=0.853000\n",
      "Epoch 4 ; Time: 1.388201 ; Training: accuracy=0.802723 ; Validation: accuracy=0.878000\n",
      "Epoch 5 ; Time: 1.702730 ; Training: accuracy=0.818977 ; Validation: accuracy=0.879500\n",
      "Epoch 6 ; Time: 2.015752 ; Training: accuracy=0.834323 ; Validation: accuracy=0.892500\n",
      "Epoch 7 ; Time: 2.331300 ; Training: accuracy=0.841337 ; Validation: accuracy=0.905333\n",
      "Epoch 8 ; Time: 2.702904 ; Training: accuracy=0.854703 ; Validation: accuracy=0.899333\n",
      "Epoch 9 ; Time: 3.065227 ; Training: accuracy=0.856931 ; Validation: accuracy=0.903667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.13630319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.453346 ; Training: accuracy=0.531498 ; Validation: accuracy=0.747150\n",
      "Epoch 2 ; Time: 0.920763 ; Training: accuracy=0.792328 ; Validation: accuracy=0.843058\n",
      "Epoch 3 ; Time: 1.348128 ; Training: accuracy=0.850281 ; Validation: accuracy=0.869551\n",
      "Epoch 4 ; Time: 1.782881 ; Training: accuracy=0.883102 ; Validation: accuracy=0.896546\n",
      "Epoch 5 ; Time: 2.219829 ; Training: accuracy=0.903687 ; Validation: accuracy=0.904427\n",
      "Epoch 6 ; Time: 2.670135 ; Training: accuracy=0.916832 ; Validation: accuracy=0.916499\n",
      "Epoch 7 ; Time: 3.106123 ; Training: accuracy=0.923198 ; Validation: accuracy=0.922200\n",
      "Epoch 8 ; Time: 3.539706 ; Training: accuracy=0.930638 ; Validation: accuracy=0.918511\n",
      "Epoch 9 ; Time: 4.004550 ; Training: accuracy=0.938492 ; Validation: accuracy=0.930919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 1.647548 ; Training: accuracy=0.433350 ; Validation: accuracy=0.677946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.317426 ; Training: accuracy=0.287609 ; Validation: accuracy=0.607227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.357903 ; Training: accuracy=0.304596 ; Validation: accuracy=0.557561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 4.715902 ; Training: accuracy=0.505554 ; Validation: accuracy=0.732335\n",
      "Epoch 2 ; Time: 9.728642 ; Training: accuracy=0.643153 ; Validation: accuracy=0.788863\n",
      "Epoch 3 ; Time: 14.775184 ; Training: accuracy=0.678714 ; Validation: accuracy=0.815108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 1.174318 ; Training: accuracy=0.045988 ; Validation: accuracy=0.068884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.877352 ; Training: accuracy=0.599189 ; Validation: accuracy=0.779153\n",
      "Epoch 2 ; Time: 1.755131 ; Training: accuracy=0.824299 ; Validation: accuracy=0.830350\n",
      "Epoch 3 ; Time: 2.788160 ; Training: accuracy=0.876107 ; Validation: accuracy=0.876025\n",
      "Epoch 4 ; Time: 3.816100 ; Training: accuracy=0.903170 ; Validation: accuracy=0.894429\n",
      "Epoch 5 ; Time: 4.851447 ; Training: accuracy=0.918232 ; Validation: accuracy=0.917350\n",
      "Epoch 6 ; Time: 5.929400 ; Training: accuracy=0.924439 ; Validation: accuracy=0.904300\n",
      "Epoch 7 ; Time: 6.983368 ; Training: accuracy=0.934288 ; Validation: accuracy=0.927388\n",
      "Epoch 8 ; Time: 8.082763 ; Training: accuracy=0.946619 ; Validation: accuracy=0.921031\n",
      "Epoch 9 ; Time: 9.148064 ; Training: accuracy=0.952744 ; Validation: accuracy=0.929396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.589238 ; Training: accuracy=0.618469 ; Validation: accuracy=0.732498\n",
      "Epoch 2 ; Time: 1.157360 ; Training: accuracy=0.743800 ; Validation: accuracy=0.752715\n",
      "Epoch 3 ; Time: 1.725213 ; Training: accuracy=0.770420 ; Validation: accuracy=0.741186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 1.752632 ; Training: accuracy=0.538882 ; Validation: accuracy=0.739862\n",
      "Epoch 2 ; Time: 3.661404 ; Training: accuracy=0.755859 ; Validation: accuracy=0.819788\n",
      "Epoch 3 ; Time: 5.574539 ; Training: accuracy=0.820207 ; Validation: accuracy=0.846206\n",
      "Epoch 4 ; Time: 7.484250 ; Training: accuracy=0.850021 ; Validation: accuracy=0.888440\n",
      "Epoch 5 ; Time: 9.477233 ; Training: accuracy=0.870973 ; Validation: accuracy=0.886758\n",
      "Epoch 6 ; Time: 11.510600 ; Training: accuracy=0.884720 ; Validation: accuracy=0.905771\n",
      "Epoch 7 ; Time: 13.443718 ; Training: accuracy=0.891843 ; Validation: accuracy=0.917887\n",
      "Epoch 8 ; Time: 15.410727 ; Training: accuracy=0.898882 ; Validation: accuracy=0.906781\n",
      "Epoch 9 ; Time: 17.363517 ; Training: accuracy=0.906418 ; Validation: accuracy=0.926468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting GP model\n",
      "BO Algorithm: Generating initial candidates.\n",
      "BO Algorithm: Scoring (and reordering) candidates.\n",
      "BO Algorithm: Selecting final set of candidates.\n",
      "Current best is [0.06908115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Time: 0.784555 ; Training: accuracy=0.533339 ; Validation: accuracy=0.728337\n",
      "Epoch 2 ; Time: 1.574606 ; Training: accuracy=0.770801 ; Validation: accuracy=0.811141\n",
      "Epoch 3 ; Time: 2.325537 ; Training: accuracy=0.837230 ; Validation: accuracy=0.848110\n",
      "Epoch 4 ; Time: 3.113237 ; Training: accuracy=0.862513 ; Validation: accuracy=0.865005\n",
      "Epoch 5 ; Time: 3.900196 ; Training: accuracy=0.884739 ; Validation: accuracy=0.869187\n",
      "Epoch 6 ; Time: 4.607019 ; Training: accuracy=0.895976 ; Validation: accuracy=0.890097\n",
      "Epoch 7 ; Time: 5.396503 ; Training: accuracy=0.908370 ; Validation: accuracy=0.902476\n",
      "Epoch 8 ; Time: 6.149938 ; Training: accuracy=0.914401 ; Validation: accuracy=0.890264\n",
      "Epoch 9 ; Time: 6.907515 ; Training: accuracy=0.924151 ; Validation: accuracy=0.910171\n"
     ]
    }
   ],
   "source": [
    "if SCHEDULER == 'fifo': \n",
    "    myscheduler = ag.scheduler.FIFOScheduler(\n",
    "        run_mlp_openml,\n",
    "        resource=resources,\n",
    "        searcher=SEARCHER,\n",
    "        time_out=120,\n",
    "        time_attr=RESOURCE_ATTR_NAME,\n",
    "        reward_attr=REWARD_ATTR_NAME)\n",
    "\n",
    "else:\n",
    "    # This setup uses rung levels at 1, 3, 9 epochs. We just use a single\n",
    "    # bracket, so this is in fact successive halving (Hyperband would use\n",
    "    # more than 1 bracket).\n",
    "    # Also note that since we do not use the max_t argument of\n",
    "    # HyperbandScheduler, this value is obtained from train_fn.args.epochs.\n",
    "    sch_type = 'stopping' if SCHEDULER == 'hbs' else 'promotion'\n",
    "    myscheduler = ag.scheduler.HyperbandScheduler(\n",
    "        run_mlp_openml,\n",
    "        resource=resources,\n",
    "        searcher=SEARCHER,\n",
    "        time_out=120,\n",
    "        time_attr=RESOURCE_ATTR_NAME,\n",
    "        reward_attr=REWARD_ATTR_NAME,\n",
    "        type=sch_type,\n",
    "        grace_period=1,\n",
    "        reduction_factor=3,\n",
    "        brackets=1)\n",
    "\n",
    "# run tasks\n",
    "myscheduler.run()\n",
    "myscheduler.join_jobs()\n",
    "\n",
    "results_df = process_training_history(\n",
    "                myscheduler.training_history.copy(),\n",
    "                start_timestamp=myscheduler._start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the results\n",
    "\n",
    "The training history is stored in the `results_df`, the main fields are the\n",
    "runtime and `'best'` (the objective).\n",
    "\n",
    "**Note**: You will get slightly different curves for different pairs of scheduler/searcher, the `time_out` here is a bit\n",
    "too short to really see the difference in a significant way (it would be better\n",
    "to set it to >1000s). Generally speaking though, hyperband stopping / promotion\n",
    "+ model will tend to significantly outperform other combinations given enough time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "40"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bracket</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>epoch</th>\n",
       "      <th>error</th>\n",
       "      <th>eval_time</th>\n",
       "      <th>objective</th>\n",
       "      <th>runtime</th>\n",
       "      <th>searcher_data_size</th>\n",
       "      <th>searcher_params_kernel_covariance_scale</th>\n",
       "      <th>searcher_params_kernel_inv_bw0</th>\n",
       "      <th>...</th>\n",
       "      <th>searcher_params_kernel_inv_bw7</th>\n",
       "      <th>searcher_params_kernel_inv_bw8</th>\n",
       "      <th>searcher_params_mean_mean_value</th>\n",
       "      <th>searcher_params_noise_variance</th>\n",
       "      <th>target_epoch</th>\n",
       "      <th>task_id</th>\n",
       "      <th>time_since_start</th>\n",
       "      <th>time_step</th>\n",
       "      <th>time_this_iter</th>\n",
       "      <th>best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.598014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.592961</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.709945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.712558</td>\n",
       "      <td>1.592113e+09</td>\n",
       "      <td>0.643653</td>\n",
       "      <td>0.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.174040</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344753</td>\n",
       "      <td>0.570004</td>\n",
       "      <td>0.655247</td>\n",
       "      <td>1.285971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.287654</td>\n",
       "      <td>1.592113e+09</td>\n",
       "      <td>0.576003</td>\n",
       "      <td>0.344753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.742718</td>\n",
       "      <td>3</td>\n",
       "      <td>0.305314</td>\n",
       "      <td>0.565562</td>\n",
       "      <td>0.694686</td>\n",
       "      <td>1.854649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.856187</td>\n",
       "      <td>1.592113e+09</td>\n",
       "      <td>0.568677</td>\n",
       "      <td>0.305314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.290722</td>\n",
       "      <td>4</td>\n",
       "      <td>0.288937</td>\n",
       "      <td>0.544203</td>\n",
       "      <td>0.711063</td>\n",
       "      <td>2.402653</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.403969</td>\n",
       "      <td>1.592113e+09</td>\n",
       "      <td>0.548004</td>\n",
       "      <td>0.288937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.840082</td>\n",
       "      <td>5</td>\n",
       "      <td>0.273061</td>\n",
       "      <td>0.546539</td>\n",
       "      <td>0.726939</td>\n",
       "      <td>2.952013</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.953363</td>\n",
       "      <td>1.592113e+09</td>\n",
       "      <td>0.549361</td>\n",
       "      <td>0.273061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bracket  elapsed_time  epoch     error  eval_time  objective   runtime  \\\n",
       "0        0      0.598014      1  0.468750   0.592961   0.531250  0.709945   \n",
       "1        0      1.174040      2  0.344753   0.570004   0.655247  1.285971   \n",
       "2        0      1.742718      3  0.305314   0.565562   0.694686  1.854649   \n",
       "3        0      2.290722      4  0.288937   0.544203   0.711063  2.402653   \n",
       "4        0      2.840082      5  0.273061   0.546539   0.726939  2.952013   \n",
       "\n",
       "   searcher_data_size  searcher_params_kernel_covariance_scale  \\\n",
       "0                 NaN                                      1.0   \n",
       "1                 1.0                                      1.0   \n",
       "2                 1.0                                      1.0   \n",
       "3                 2.0                                      1.0   \n",
       "4                 2.0                                      1.0   \n",
       "\n",
       "   searcher_params_kernel_inv_bw0  ...  searcher_params_kernel_inv_bw7  \\\n",
       "0                             1.0  ...                             1.0   \n",
       "1                             1.0  ...                             1.0   \n",
       "2                             1.0  ...                             1.0   \n",
       "3                             1.0  ...                             1.0   \n",
       "4                             1.0  ...                             1.0   \n",
       "\n",
       "   searcher_params_kernel_inv_bw8  searcher_params_mean_mean_value  \\\n",
       "0                             1.0                              0.0   \n",
       "1                             1.0                              0.0   \n",
       "2                             1.0                              0.0   \n",
       "3                             1.0                              0.0   \n",
       "4                             1.0                              0.0   \n",
       "\n",
       "   searcher_params_noise_variance  target_epoch  task_id  time_since_start  \\\n",
       "0                           0.001             9        0          0.712558   \n",
       "1                           0.001             9        0          1.287654   \n",
       "2                           0.001             9        0          1.856187   \n",
       "3                           0.001             9        0          2.403969   \n",
       "4                           0.001             9        0          2.953363   \n",
       "\n",
       "      time_step  time_this_iter      best  \n",
       "0  1.592113e+09        0.643653  0.468750  \n",
       "1  1.592113e+09        0.576003  0.344753  \n",
       "2  1.592113e+09        0.568677  0.305314  \n",
       "3  1.592113e+09        0.548004  0.288937  \n",
       "4  1.592113e+09        0.549361  0.273061  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "42"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Objective')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "runtime = results_df['runtime'].values\n",
    "objective = results_df['best'].values\n",
    "\n",
    "plt.plot(runtime, objective, lw=2)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlim(0, 120)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel(\"Runtime [s]\", fontsize=14)\n",
    "plt.ylabel(\"Objective\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Deeper\n",
    "\n",
    "Now, you are ready to try HPO on your own machine learning models (if you use\n",
    "PyTorch, have a look at :ref:`sec_customstorch`). While AutoGluon comes with\n",
    "well-chosen defaults, it can pay off to tune it to your specific needs. Here are\n",
    "some tips which may come useful.\n",
    "\n",
    "### Logging the Search Progress\n",
    "\n",
    "First, it is a good idea in general to switch on `debug_log`, which outputs\n",
    "useful information about the search progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "search_options: Key 'random_seed': Imputing default value 31415927\n",
      "search_options: Key 'opt_skip_init_length': Imputing default value 150\n",
      "search_options: Key 'opt_skip_period': Imputing default value 1\n",
      "search_options: Key 'profiler': Imputing default value False\n",
      "search_options: Key 'opt_maxiter': Imputing default value 50\n",
      "search_options: Key 'opt_nstarts': Imputing default value 2\n",
      "search_options: Key 'opt_warmstart': Imputing default value False\n",
      "search_options: Key 'opt_verbose': Imputing default value False\n",
      "search_options: Key 'opt_debug_writer': Imputing default value False\n",
      "search_options: Key 'num_fantasy_samples': Imputing default value 20\n",
      "search_options: Key 'num_init_random': Imputing default value 5\n",
      "search_options: Key 'num_init_candidates': Imputing default value 250\n",
      "search_options: Key 'initial_scoring': Imputing default value thompson_indep\n",
      "search_options: Key 'first_is_default': Imputing default value True\n",
      "\n",
      "[GPFIFOSearcher.__init__]\n",
      "- acquisition_class = <class 'autogluon.searcher.bayesopt.models.nphead_acqfunc.EIAcquisitionFunction'>\n",
      "- local_minimizer_class = <class 'autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components.LBFGSOptimizeAcquisition'>\n",
      "- num_initial_candidates = 250\n",
      "- num_initial_random_choices = 5\n",
      "- initial_scoring = thompson_indep\n",
      "- first_is_default = True\n"
     ]
    }
   ],
   "source": [
    "myscheduler = ag.scheduler.FIFOScheduler(\n",
    "    run_mlp_openml,\n",
    "    resource=resources,\n",
    "    searcher=SEARCHER,\n",
    "    search_options={'debug_log': True},\n",
    "    time_out=120,\n",
    "    time_attr=RESOURCE_ATTR_NAME,\n",
    "    reward_attr=REWARD_ATTR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs show which configurations are chosen, stopped, or promoted. For\n",
    "BO and BOHB, a range of information is displayed for every `get_config`\n",
    "decision. This log output is very useful in order to figure out what is going\n",
    "on during the search.\n",
    "\n",
    "### Configuring `HyperbandScheduler`\n",
    "\n",
    "The most important knobs to turn with `HyperbandScheduler` are `max_t`, `grace_period`,\n",
    "`reduction_factor`, `brackets`, and `type`. The first three determine the rung\n",
    "levels at which stopping or promotion decisions are being made.\n",
    "\n",
    "- The maximum resource level `max_t` (usually, resource equates to epochs, so\n",
    "   `max_t` is the maximum number of training epochs) is typically hardcoded in\n",
    "   `train_fn` passed to the scheduler (this is `run_mlp_openml` in the example\n",
    "   above). As already noted above, the value is best fixed in the `ag.args`\n",
    "   decorator as `epochs=XYZ`, it can then be accessed as `args.epochs` in the\n",
    "   `train_fn` code. If this is done, you do not have to pass `max_t` when creating\n",
    "   the scheduler.\n",
    "- `grace_period` and `reduction_factor` determine the rung levels, which are\n",
    "   `grace_period`, `grace_period * reduction_factor`,\n",
    "   `grace_period * (reduction_factor ** 2)`, etc. All rung levels must be less or\n",
    "   equal than `max_t`. It is recommended to make `max_t` equal to the largest rung\n",
    "   level. For example, if `grace_period = 1`, `reduction_factor = 3`, it is in\n",
    "   general recommended to use `max_t = 9`, `max_t = 27`, or `max_t = 81`. Choosing\n",
    "   a `max_t` value \"off the grid\" works against the successive halving principle\n",
    "   that the total resources spent in a rung should be roughly equal between rungs. If in the\n",
    "   example above, you set `max_t = 10`, about a third of configurations reaching\n",
    "   9 epochs are allowed to proceed, but only for one more epoch.\n",
    "- With `reduction_factor`, you tune the extent to which successive halving\n",
    "   filtering is applied. The larger this integer, the fewer configurations make\n",
    "   it to higher number of epochs. Values 2, 3, 4 are commonly used.\n",
    "- Finally, `grace_period` should be set to the smallest resource (number of epochs)\n",
    "   for which you expect any meaningful differentiation between configurations.\n",
    "   While `grace_period = 1` should always be explored, it may be too low for any\n",
    "   meaningful stopping decisions to be made at the first rung.\n",
    "- `brackets` sets the maximum number of brackets in Hyperband (make sure to study\n",
    "   the Hyperband paper or follow-ups for details). For `brackets = 1`, you are\n",
    "   running successive halving (single bracket). Higher brackets have larger effective\n",
    "   `grace_period` values (so runs are not stopped until later), yet are also chosen\n",
    "   with less probability. We recommend to always consider successive halving\n",
    "   (`brackets = 1`) in a comparison.\n",
    "- Finally, with `type` (values `stopping`, `promotion`) you are choosing different\n",
    "   ways of extending successive halving scheduling to the asynchronous\n",
    "   case. The method for the default `stopping` is simpler and seems to perform well,\n",
    "   but `promotion` is more careful promoting configurations to higher resource\n",
    "   levels, which can work better in some cases.\n",
    "\n",
    "### Asynchronous BOHB\n",
    "\n",
    "Finally, here are some ideas for tuning asynchronous BOHB, apart from tuning its\n",
    "`HyperbandScheduling` component. You need to pass these options in `search_options`.\n",
    "\n",
    "- We support a range of different surrogate models over the criterion functions\n",
    "   across resource levels. All of them are jointly dependent Gaussian process\n",
    "   models, meaning that data collected at all resource levels are modelled\n",
    "   together. The surrogate model is selected by `gp_resource_kernel`, values are\n",
    "   `matern52`, `matern52-res-warp`, `exp-decay-sum`, `exp-decay-combined`,\n",
    "   `exp-decay-delta1`. These are variants of either a joint Matern 5/2 kernel\n",
    "   over configuration and resource, or the exponential decay model. Details about\n",
    "   the latter can be found [here](https://arxiv.org/abs/2003.10865).\n",
    "- Fitting a Gaussian process surrogate model to data encurs a cost which scales\n",
    "   cubically with the number of datapoints. When applied to expensive deep learning\n",
    "   workloads, even multi-fidelity asynchronous BOHB is rarely running up more than\n",
    "   100 observations or so (across all rung levels and brackets), and the GP\n",
    "   computations are subdominant. However, if you apply it to cheaper `train_fn`\n",
    "   and find yourself beyond 2000 total evaluations, the cost of GP fitting can\n",
    "   become painful. In such a situation, you can explore the options `opt_skip_period`\n",
    "   and `opt_skip_num_max_resource`. The basic idea is as follows. By far the most\n",
    "   expensive part of a `get_config` call (picking the next configuration) is the\n",
    "   refitting of the GP model to past data (this entails re-optimizing hyperparameters\n",
    "   of the surrogate model itself). The options allow you to skip this expensive\n",
    "   step for most `get_config` calls, after some initial period. Check the docstrings\n",
    "   for details about these options. If you find yourself in such a situation and\n",
    "   gain experience with these skipping features, make sure to contact the AutoGluon\n",
    "   developers -- we would love to learn about your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mxnet_p36] *",
   "language": "python",
   "name": "conda-env-mxnet_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
